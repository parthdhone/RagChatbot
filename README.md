# ğŸ¤– Realtime RAG-based Chatbot using ChromaDB, Mistral (GGUF), and Gemini API

A full-stack Retrieval-Augmented Generation (RAG) based chatbot system powered by **ChromaDB**, **Mistral 7B (GGUF)** for local LLM inference, and **Gemini API** for document summarization. The project features a custom HTML/CSS frontend with support for dynamic PDF upload and chatbot querying.

---

## ğŸ“ Project Structure

Realtime_ChatBot/

â”‚
â”œâ”€â”€ backend/

â”‚ â”œâ”€â”€ app.py # FastAPI backend API (serves chatbot) 
â”‚ â”œâ”€â”€ chain.py # Core RAG logic (embedding + LLM response)
â”‚ â”œâ”€â”€ ingest.py # Index and embed documents into ChromaDB
â”‚
â”‚

â”œâ”€â”€ data/

â”‚ â””â”€â”€ pdfs/ # Initial PDF documents (ABB, Acer, Realme, etc.)
â”‚ â””â”€â”€chromadb/ # create embeddings and main database

â”œâ”€â”€ notebooks/

â”‚ â”œâ”€â”€ chromadb # Custom chatbot UI
â”‚ â”œâ”€â”€ experiments.ipynb # Tested and Debugging Code for chatbot
â”‚
â”œâ”€â”€ uploads/ # Uploaded PDF files for custom queries
â”œâ”€â”€ models/ # Local Mistral model (.gguf format)
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # You are here!


---

## ğŸ’¡ Features

- ğŸ“„ PDF ingestion and semantic chunking
- ğŸ§  ChromaDB as vector database (persistent)
- ğŸ’¬ Local inference using **Mistral-7B-Instruct** `.gguf` via llama.cpp / ctransformers
- â˜ï¸ Gemini API for PDF summarization during upload
- ğŸŒ FastAPI backend with REST endpoint
- ğŸ¨ Custom HTML/CSS frontend interface
- ğŸ“ Dynamic PDF Upload and Question Answering

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/your-username/realtime-rag-chatbot.git
cd realtime-rag-chatbot


2. Create and Activate Virtual Environment

python -m venv rag_env
rag_env\Scripts\activate   # Windows
# source rag_env/bin/activate   # Linux/Mac

3. Install Requirements

pip install -r requirements.txt


ğŸ§  Ingest and Index PDF Manuals

python backend/ingest.py

This stores embeddings in persistent ChromaDB located in backend/chromadb.

ğŸš€ Run the FastAPI Backend

uvicorn backend.app:app --reload

Visit FastAPI docs: http://localhost:8000/docs

Run the Frontend (HTML/CSS)

Simply open frontend/index.html in your browser.

You can serve it with a static server like VSCode Live Server or Python's HTTP server:

cd frontend
python -m http.server 5500

ğŸ” How it Works
RAG Pipeline
ğŸ§© Embedding: all-MiniLM-L6-v2 from Sentence Transformers

ğŸ“š Storage: ChromaDB (persistent vector store)

ğŸ¤– LLM: mistralai/Mistral-7B-Instruct-v0.1 in .gguf format via ctransformers

ğŸ’¬ Response: Top-k relevant chunks + query passed to LLM for generation

ğŸ“ Gemini API: Used to summarize uploaded PDFs before ingestion

ğŸ“¤ PDF Upload + Gemini Summarization
Upload a PDF using the Upload Button in the frontend.

Gemini API extracts and summarizes content.

Summary is embedded and added to the ChromaDB vector store.



ğŸ§° Tech Stack
Component----	Tool / Library
Embeddings---	sentence-transformers (MiniLM)
Vector DB---	ChromaDB
LLM Inference---	ctransformers with .gguf Mistral LLM
Summarization---	Google Gemini API
Backend API	FastAPI
Frontend---	HTML + CSS

ğŸ“¦ Dependencies
txt
Copy
Edit
fastapi
uvicorn
chromadb
ctransformers
sentence-transformers
PyMuPDF
requests
openai
python-dotenv

ğŸ“Œ Note: You must manually download the .gguf Mistral model and place it inside the models/ folder.


â“ Sample Queries

â€œHow to pair Realme Buds with phone?â€

â€œWhat is the voltage range supported by ABB device?â€

â€œHow to reset a Dell Latitude laptop?â€

ğŸ› ï¸ Troubleshooting
âœ… Make sure ChromaDB directory exists before querying.

âœ… Verify Mistral .gguf model path is correct in chain.py.

âœ… Check tokenizer and generator are properly loaded with correct model path.

âœ… If UI freezes, check browser console for network errors.

Contact
Made with â¤ï¸ by Parth Dhone

For issues or suggestions, open a GitHub issue or email at "parthdhone53@example.com"

Github Link:




